{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "agreed-korean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[ 0.2745, -0.6022]], requires_grad=True)]\n",
      "Epoch 0 - loss: 15.922136306762695\n",
      "Epoch 0 - loss: 46.4136848449707\n",
      "Epoch 0 - loss: 43.84897232055664\n",
      "Epoch 0 - loss: 29.500673294067383\n",
      "Epoch 0 - loss: 20.48428726196289\n",
      "Epoch 0 - loss: 9.803570747375488\n",
      "Epoch 1 - loss: 1.4389972686767578\n",
      "Epoch 1 - loss: 17.290185928344727\n",
      "Epoch 1 - loss: 28.055355072021484\n",
      "Epoch 1 - loss: 22.537437438964844\n",
      "Epoch 1 - loss: 1.1380115747451782\n",
      "Epoch 1 - loss: 13.927228927612305\n",
      "Epoch 2 - loss: 0.9474205374717712\n",
      "Epoch 2 - loss: 11.663762092590332\n",
      "Epoch 2 - loss: 16.04863929748535\n",
      "Epoch 2 - loss: 10.362236022949219\n",
      "Epoch 2 - loss: 0.004177718888968229\n",
      "Epoch 2 - loss: 8.73387336730957\n",
      "Epoch 3 - loss: 0.4971398711204529\n",
      "Epoch 3 - loss: 5.7752838134765625\n",
      "Epoch 3 - loss: 6.7062458992004395\n",
      "Epoch 3 - loss: 3.3730480670928955\n",
      "Epoch 3 - loss: 0.1565811187028885\n",
      "Epoch 3 - loss: 3.9119884967803955\n",
      "Epoch 4 - loss: 0.18985874950885773\n",
      "Epoch 4 - loss: 2.06640625\n",
      "Epoch 4 - loss: 1.9506454467773438\n",
      "Epoch 4 - loss: 0.6631211042404175\n",
      "Epoch 4 - loss: 0.27868014574050903\n",
      "Epoch 4 - loss: 1.2432074546813965\n",
      "Epoch 5 - loss: 0.04863927885890007\n",
      "Epoch 5 - loss: 0.47843238711357117\n",
      "Epoch 5 - loss: 0.3063487112522125\n",
      "Epoch 5 - loss: 0.02913656085729599\n",
      "Epoch 5 - loss: 0.23090337216854095\n",
      "Epoch 5 - loss: 0.23378100991249084\n",
      "Epoch 6 - loss: 0.005614068824797869\n",
      "Epoch 6 - loss: 0.0407998189330101\n",
      "Epoch 6 - loss: 0.0022084556985646486\n",
      "Epoch 6 - loss: 0.026894915848970413\n",
      "Epoch 6 - loss: 0.12914490699768066\n",
      "Epoch 6 - loss: 0.008091103285551071\n",
      "Epoch 7 - loss: 6.859063432784751e-05\n",
      "Epoch 7 - loss: 0.004560128785669804\n",
      "Epoch 7 - loss: 0.039351027458906174\n",
      "Epoch 7 - loss: 0.08174051344394684\n",
      "Epoch 7 - loss: 0.05238209292292595\n",
      "Epoch 7 - loss: 0.012482442893087864\n",
      "Epoch 8 - loss: 0.0020513131748884916\n",
      "Epoch 8 - loss: 0.03204386681318283\n",
      "Epoch 8 - loss: 0.0737793892621994\n",
      "Epoch 8 - loss: 0.0804607942700386\n",
      "Epoch 8 - loss: 0.014627456665039062\n",
      "Epoch 8 - loss: 0.03331490978598595\n",
      "Epoch 9 - loss: 0.0028271719347685575\n",
      "Epoch 9 - loss: 0.037620555609464645\n",
      "Epoch 9 - loss: 0.06234274432063103\n",
      "Epoch 9 - loss: 0.05022342503070831\n",
      "Epoch 9 - loss: 0.002095563104376197\n",
      "Epoch 9 - loss: 0.031619347631931305\n",
      "Epoch 10 - loss: 0.0021039973944425583\n",
      "Epoch 10 - loss: 0.025891609489917755\n",
      "Epoch 10 - loss: 0.035334933549165726\n",
      "Epoch 10 - loss: 0.022551700472831726\n",
      "Epoch 10 - loss: 1.854627043940127e-06\n",
      "Epoch 10 - loss: 0.019307728856801987\n",
      "Epoch 11 - loss: 0.001089754980057478\n",
      "Epoch 11 - loss: 0.012620697729289532\n",
      "Epoch 11 - loss: 0.014518087729811668\n",
      "Epoch 11 - loss: 0.007190353237092495\n",
      "Epoch 11 - loss: 0.00038363345083780587\n",
      "Epoch 11 - loss: 0.008502474054694176\n",
      "Epoch 12 - loss: 0.000408859719755128\n",
      "Epoch 12 - loss: 0.0044321101158857346\n",
      "Epoch 12 - loss: 0.004126836080104113\n",
      "Epoch 12 - loss: 0.0013606632128357887\n",
      "Epoch 12 - loss: 0.0006397053366526961\n",
      "Epoch 12 - loss: 0.002646000124514103\n",
      "Epoch 13 - loss: 0.00010196439689025283\n",
      "Epoch 13 - loss: 0.00099509721621871\n",
      "Epoch 13 - loss: 0.0006157646421343088\n",
      "Epoch 13 - loss: 4.881283166469075e-05\n",
      "Epoch 13 - loss: 0.0005167772178538144\n",
      "Epoch 13 - loss: 0.00047786495997570455\n",
      "Epoch 14 - loss: 1.0919598025793675e-05\n",
      "Epoch 14 - loss: 7.638669922016561e-05\n",
      "Epoch 14 - loss: 1.8261644072481431e-06\n",
      "Epoch 14 - loss: 7.003165956120938e-05\n",
      "Epoch 14 - loss: 0.0002840026863850653\n",
      "Epoch 14 - loss: 1.2871754734078422e-05\n",
      "Epoch 15 - loss: 2.887938990170369e-07\n",
      "Epoch 15 - loss: 1.3494995073415339e-05\n",
      "Epoch 15 - loss: 9.69009124673903e-05\n",
      "Epoch 15 - loss: 0.00019003621127922088\n",
      "Epoch 15 - loss: 0.00011321267083985731\n",
      "Epoch 15 - loss: 3.19607206620276e-05\n",
      "Epoch 16 - loss: 4.901596184936352e-06\n",
      "Epoch 16 - loss: 7.553888281108811e-05\n",
      "Epoch 16 - loss: 0.00016958349442575127\n",
      "Epoch 16 - loss: 0.00018125292262993753\n",
      "Epoch 16 - loss: 3.087032018811442e-05\n",
      "Epoch 16 - loss: 7.721409201622009e-05\n",
      "Epoch 17 - loss: 6.441270215873374e-06\n",
      "Epoch 17 - loss: 8.526586316293105e-05\n",
      "Epoch 17 - loss: 0.00013961843797005713\n",
      "Epoch 17 - loss: 0.00011103147699031979\n",
      "Epoch 17 - loss: 4.180707037448883e-06\n",
      "Epoch 17 - loss: 7.110516889952123e-05\n",
      "Epoch 18 - loss: 4.68550297227921e-06\n",
      "Epoch 18 - loss: 5.7475110224913806e-05\n",
      "Epoch 18 - loss: 7.773453398840502e-05\n",
      "Epoch 18 - loss: 4.90129241370596e-05\n",
      "Epoch 18 - loss: 1.3096723705530167e-10\n",
      "Epoch 18 - loss: 4.26259393861983e-05\n",
      "Epoch 19 - loss: 2.387614586041309e-06\n",
      "Epoch 19 - loss: 2.7557252906262875e-05\n",
      "Epoch 19 - loss: 3.140246553812176e-05\n",
      "Epoch 19 - loss: 1.5310986782424152e-05\n",
      "Epoch 19 - loss: 9.314535418525338e-07\n",
      "Epoch 19 - loss: 1.845002407208085e-05\n",
      "Epoch 20 - loss: 8.788347258814611e-07\n",
      "Epoch 20 - loss: 9.488667274126783e-06\n",
      "Epoch 20 - loss: 8.7177031673491e-06\n",
      "Epoch 20 - loss: 2.7821452022180893e-06\n",
      "Epoch 20 - loss: 1.4623074093833566e-06\n",
      "Epoch 20 - loss: 5.620855517918244e-06\n",
      "Epoch 21 - loss: 2.1327474541976699e-07\n",
      "Epoch 21 - loss: 2.065507032966707e-06\n",
      "Epoch 21 - loss: 1.2343889466137625e-06\n",
      "Epoch 21 - loss: 7.861308404244483e-08\n",
      "Epoch 21 - loss: 1.1510792319313623e-06\n",
      "Epoch 21 - loss: 9.723917173687369e-07\n",
      "Epoch 22 - loss: 2.1082144030515337e-08\n",
      "Epoch 22 - loss: 1.411863195244223e-07\n",
      "Epoch 22 - loss: 5.684341886080801e-10\n",
      "Epoch 22 - loss: 1.7929414752870798e-07\n",
      "Epoch 22 - loss: 6.235350156202912e-07\n",
      "Epoch 22 - loss: 1.9386789062991738e-08\n",
      "Epoch 23 - loss: 1.0206804290646687e-09\n",
      "Epoch 23 - loss: 3.80352958018193e-08\n",
      "Epoch 23 - loss: 2.3563279683003202e-07\n",
      "Epoch 23 - loss: 4.405737854540348e-07\n",
      "Epoch 23 - loss: 2.430979293421842e-07\n",
      "Epoch 23 - loss: 8.076676749624312e-08\n",
      "Epoch 24 - loss: 1.1562008239707211e-08\n",
      "Epoch 24 - loss: 1.7647857930569444e-07\n",
      "Epoch 24 - loss: 3.890054358635098e-07\n",
      "Epoch 24 - loss: 4.0705435822019354e-07\n",
      "Epoch 24 - loss: 6.53235474601388e-08\n",
      "Epoch 24 - loss: 1.7768252291716635e-07\n",
      "Epoch 25 - loss: 1.461154397475184e-08\n",
      "Epoch 25 - loss: 1.9244907889515162e-07\n",
      "Epoch 25 - loss: 3.1231684261001647e-07\n",
      "Epoch 25 - loss: 2.449824023642577e-07\n",
      "Epoch 25 - loss: 8.381903171539307e-09\n",
      "Epoch 25 - loss: 1.6043486539274454e-07\n",
      "Epoch 26 - loss: 1.0559290331002558e-08\n",
      "Epoch 26 - loss: 1.285807229578495e-07\n",
      "Epoch 26 - loss: 1.7130878404714167e-07\n",
      "Epoch 26 - loss: 1.0700114216888323e-07\n",
      "Epoch 26 - loss: 2.2737367544323206e-11\n",
      "Epoch 26 - loss: 9.430004865862429e-08\n",
      "Epoch 27 - loss: 5.218737442191923e-09\n",
      "Epoch 27 - loss: 6.007121555740014e-08\n",
      "Epoch 27 - loss: 6.828122423030436e-08\n",
      "Epoch 27 - loss: 3.283275873400271e-08\n",
      "Epoch 27 - loss: 2.1836967789568007e-09\n",
      "Epoch 27 - loss: 4.0108716348186135e-08\n",
      "Epoch 28 - loss: 1.9036292542295996e-09\n",
      "Epoch 28 - loss: 2.0600282368832268e-08\n",
      "Epoch 28 - loss: 1.859825715655461e-08\n",
      "Epoch 28 - loss: 5.820766091346741e-09\n",
      "Epoch 28 - loss: 3.384229785297066e-09\n",
      "Epoch 28 - loss: 1.2238160707056522e-08\n",
      "Epoch 29 - loss: 4.502567207964603e-10\n",
      "Epoch 29 - loss: 4.393086783238687e-09\n",
      "Epoch 29 - loss: 2.5547706172801554e-09\n",
      "Epoch 29 - loss: 1.3096723705530167e-10\n",
      "Epoch 29 - loss: 2.5547706172801554e-09\n",
      "Epoch 29 - loss: 2.0954757928848267e-09\n",
      "Epoch 30 - loss: 4.780531526193954e-11\n",
      "Epoch 30 - loss: 3.112745616817847e-10\n",
      "Epoch 30 - loss: 0.0\n",
      "Epoch 30 - loss: 4.0108716348186135e-10\n",
      "Epoch 30 - loss: 1.4551915228366852e-09\n",
      "Epoch 30 - loss: 3.2741809263825417e-11\n",
      "Epoch 31 - loss: 3.637978807091713e-12\n",
      "Epoch 31 - loss: 1.1004885891452432e-10\n",
      "Epoch 31 - loss: 5.684341886080801e-10\n",
      "Epoch 31 - loss: 1.051375875249505e-09\n",
      "Epoch 31 - loss: 4.81122697237879e-10\n",
      "Epoch 31 - loss: 2.3283064365386963e-10\n",
      "Epoch 32 - loss: 3.007016857736744e-11\n",
      "Epoch 32 - loss: 4.4019543565809727e-10\n",
      "Epoch 32 - loss: 9.313225746154785e-10\n",
      "Epoch 32 - loss: 9.313225746154785e-10\n",
      "Epoch 32 - loss: 1.3096723705530167e-10\n",
      "Epoch 32 - loss: 3.637978807091713e-10\n",
      "Epoch 33 - loss: 2.5067947717616335e-11\n",
      "Epoch 33 - loss: 3.637978807091713e-10\n",
      "Epoch 33 - loss: 6.630216375924647e-10\n",
      "Epoch 33 - loss: 5.684341886080801e-10\n",
      "Epoch 33 - loss: 3.2741809263825417e-11\n",
      "Epoch 33 - loss: 2.9467628337442875e-10\n",
      "Epoch 34 - loss: 2.0520474208751693e-11\n",
      "Epoch 34 - loss: 2.476099325576797e-10\n",
      "Epoch 34 - loss: 3.283275873400271e-10\n",
      "Epoch 34 - loss: 2.3283064365386963e-10\n",
      "Epoch 34 - loss: 0.0\n",
      "Epoch 34 - loss: 1.7826096154749393e-10\n",
      "Epoch 35 - loss: 8.185452315956354e-12\n",
      "Epoch 35 - loss: 1.1004885891452432e-10\n",
      "Epoch 35 - loss: 1.1004885891452432e-10\n",
      "Epoch 35 - loss: 7.366907084360719e-11\n",
      "Epoch 35 - loss: 3.637978807091713e-12\n",
      "Epoch 35 - loss: 5.820766091346741e-11\n",
      "Epoch 36 - loss: 1.4210854715202004e-12\n",
      "Epoch 36 - loss: 2.2737367544323206e-11\n",
      "Epoch 36 - loss: 2.2737367544323206e-11\n",
      "Epoch 36 - loss: 1.4551915228366852e-11\n",
      "Epoch 36 - loss: 3.637978807091713e-12\n",
      "Epoch 36 - loss: 3.2741809263825417e-11\n",
      "Epoch 37 - loss: 2.0463630789890885e-12\n",
      "Epoch 37 - loss: 1.8417267710901797e-11\n",
      "Epoch 37 - loss: 2.2737367544323206e-11\n",
      "Epoch 37 - loss: 3.637978807091713e-12\n",
      "Epoch 37 - loss: 8.185452315956354e-12\n",
      "Epoch 37 - loss: 3.2741809263825417e-11\n",
      "Epoch 38 - loss: 2.0463630789890885e-12\n",
      "Epoch 38 - loss: 1.8417267710901797e-11\n",
      "Epoch 38 - loss: 1.4551915228366852e-11\n",
      "Epoch 38 - loss: 3.637978807091713e-12\n",
      "Epoch 38 - loss: 8.185452315956354e-12\n",
      "Epoch 38 - loss: 1.4551915228366852e-11\n",
      "Epoch 39 - loss: 2.2737367544323206e-13\n",
      "Epoch 39 - loss: 2.0463630789890885e-12\n",
      "Epoch 39 - loss: 0.0\n",
      "Epoch 39 - loss: 0.0\n",
      "Epoch 39 - loss: 8.185452315956354e-12\n",
      "Epoch 39 - loss: 0.0\n",
      "when x = tensor([1.0000, 2.1000]), y = tensor([3.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([2.0000, 3.5000]), y = tensor([6.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([3., 3.]), y = tensor([9.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([4.0000, 2.1000]), y = tensor([12.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([5.0000, 7.2000]), y = tensor([15.0000], grad_fn=<SqueezeBackward4>)\n",
      "when x = tensor([ 6.0000, 10.1000]), y = tensor([18.0000], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "### Training with manually updating W with \"Backward\" ###\n",
    "\n",
    "import torch\n",
    "#from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [(1.0,2.1,3.0), (2.0, 3.5, 6.0), (3.0, 3.0, 9.0), (4.0, 2.1, 12.0), (5.0, 7.2, 15.0), (6.0, 10.1, 18.0)]\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    NN class\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 1, bias=False)\n",
    "    def forward(self, x1):\n",
    "        x = self.fc1(x1)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)\n",
    "print(list(net.parameters()))\n",
    "\n",
    "#input = torch.randn(1)\n",
    "#out = net(input)\n",
    "\n",
    "#def criterion(out, label):\n",
    "#    return (label - out)**2\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.5)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(40):\n",
    "    for i, current_data in enumerate(data):\n",
    "        X1, X2, Y = current_data\n",
    "        X, Y = torch.FloatTensor([X1, X2]), torch.FloatTensor([Y])\n",
    "        optimizer.zero_grad()   \n",
    "        outputs = net(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()    ## This line is equivalent to \"W = W - lr* W.grad\"\n",
    "        print(\"Epoch {} - loss: {}\".format(epoch, loss))\n",
    "\n",
    "### Test the trained network ###            \n",
    "for i, current_data in enumerate(data):\n",
    "    X1, X2, Y = current_data\n",
    "    X, Y = torch.FloatTensor([X1, X2]), torch.FloatTensor([Y])\n",
    "\n",
    "    out = net(torch.FloatTensor(X))\n",
    "    #out = net(torch.FloatTensor(X1), torch.FloatTensor(X2))  \n",
    "    print(\"when x = {}, y = {}\".format(X, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9510f4-8f4e-4afd-a34b-059e0d17c7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
